# -*- coding: utf-8 -*-
"""MovieBonus.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1urUUFeIU-G7yTfhKrV0TWftKqANPJOs0
"""

import pandas as pd

df = pd.read_csv('/content/test_data (1).csv')
display(df.head())

"""## Data preprocessing

Extract the text and labels, tokenize the text, pad the sequences, determine vocabulary size and embedding dimension.
"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 1. Extract text data and labels
texts = df.iloc[:, 0].values
labels = df.iloc[:, 1].values

# 2. Tokenize the text data
max_words = 10000
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

# 3. Determine the maximum sequence length
max_sequence_length = max([len(x) for x in sequences])

# 4. Pad the sequences
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', truncating='post')

# 5. Define vocabulary size
vocab_size = len(tokenizer.word_index) + 1 # Add 1 for the OOV token

# 6. Determine embedding dimension (a common starting point)
embedding_dim = 100

print(f"Max Sequence Length: {max_sequence_length}")
print(f"Vocabulary Size: {vocab_size}")
print(f"Embedding Dimension: {embedding_dim}")
print(f"Shape of padded sequences: {padded_sequences.shape}")

"""## Model building

Import the necessary layers from tensorflow.keras and build the LSTM model architecture as specified in the instructions.
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# 2. Initialize a Sequential model.
model = Sequential()

# 3. Add an Embedding layer
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))

# 4. Add an LSTM layer
model.add(LSTM(units=128)) # Using 128 units as an example

# 5. Add a Dense layer
model.add(Dense(units=1, activation='sigmoid'))

model.summary()

"""## Model compilation

Compile the model with binary crossentropy loss, adam optimizer, and accuracy metric.
"""

# 1. Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

"""## Model training

The subtask requires splitting the data into training and testing sets and then training the model. These steps can be combined into a single code block.
"""

from sklearn.model_selection import train_test_split

# 1. Split the padded_sequences and labels into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# 2. Train the compiled model using the training data.
# 3. Store the training history in a variable.
history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

"""## Model evaluation

Evaluate the trained model on the test data using the evaluate method and print the results.
"""

# Evaluate the trained model on the test data
evaluation_results = model.evaluate(X_test, y_test, verbose=0)

# Print the evaluation results
print(f"Test Loss: {evaluation_results[0]}")
print(f"Test Accuracy: {evaluation_results[1]}")